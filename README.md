# ğ…ğ¨ğ¨ğ­ğ›ğšğ¥ğ¥ ğğšğ­ğšğ¬ğğ­ ğšğ§ğšğ¥ğ²ğ¬ğ¢ğ¬ ğ°ğ¢ğ­ğ¡ ğ©ğ¨ğ¬ğ­ğ ğ«ğğ¬ ğšğ§ğ ğ©ğ²ğ­ğ¡ğ¨ğ§ ğ©ğšğ§ğğšğ¬  

![Project Architecture](https://github.com/Androjerson/football_dataset_analysis_postgres/blob/main/Assets/Project%20Architecture.gif)

## Project Description:
This project involves working with comprehensive football dataset covering the Top 5 leagues in Europe from 2014-2020. The analytics team aims to perform some major analysis such as the top players who have scored the most goals.
 
As as a data engineer, my role involved:
1)ğƒğšğ­ğš ğ„ğ±ğ­ğ«ğšğœğ­ğ¢ğ¨ğ§ :Sourcing the data in ğ˜Šğ˜šğ˜ format
2)ğƒğšğ­ğš ğ¦ğšğ§ğ¢ğ©ğ®ğ¥ğšğ­ğ¢ğ¨ğ§ ğšğ§ğ ğ‚ğ¥ğğšğ§ğ¢ğ§ğ :Using ğ˜±ğ˜¢ğ˜¯ğ˜¥ğ˜¢ğ˜´ library in python
3)ğƒğšğ­ğš ğŒğ¨ğğğ¥ğ¥ğ¢ğ§ğ :Building data models based on the source and developing SQL queries
4)ğƒğšğ­ğš ğ‹ğ¨ğšğğ¢ğ§ğ :Loading the cleaned and modelled data into ğ˜—ğ˜°ğ˜´ğ˜µğ˜¨ğ˜³ğ˜¦ğ˜šğ˜˜ğ˜“ using ğ˜±ğ˜´ğ˜ºğ˜¤ğ˜°ğ˜±ğ˜¨2 adapter

This project involves loading CSV files into a relational database for analysis which leverages the strengths of database systems in ğğšğ­ğš ğ¢ğ§ğ­ğğ ğ«ğ¢ğ­ğ², ğ©ğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ ğšğ§ğ ğšğğ¯ğšğ§ğœğğ ğªğ®ğğ«ğ²ğ¢ğ§ğ  . This approach gives us upperhand in analysing data over direct CSV analysis ,making it a superior choice for managing and analyzing large and complex datasets

## Tools/Programming Languages used:

1. Python
2. PostgreSQL
3. psycopg2 python PoastgreSQL connector
4. pandas python library
5. AWS secrets manager
6. Boto3 AWS python SDK 

## Dataset:

1. Players
2. Teams
3. Games
4. Shots
5. Leagues
6. TeamStats
7. PlayerStats

## Schema:
![Schema Design](https://github.com/Androjerson/football_dataset_analysis_postgres/blob/main/Assets/schema_design.png)

### Dimension Tables:
* players:Basic players information
    id,name
* teams:Basic teams information
    id,name
* leagues:Basic leagues information with League name in understat notation
    id,name,u_notation
* games:Detailed game information with team ids,goals,goal probability
    id,league_id,season,date,homeTeam_id,awayTeam_id,home_goals,away_goals,home_prob,draw_probability

### Fact Tables:
  * shots:Statistical information of each shot targeting goal played in the game
     game_id,shooter_id,assister_id,minute,situation,lastAction,shotType,shotResult,xpct_goals,positionX,positionY
  * team_stats:Statistical information of each team
      game_id,team_id,season,date,loc_indc,goals,xpct_goals,shots,shotsOnTarget,deep
  * player_stats:Statistical information of each player
      game_id,player_id,goals,own_goal,shots,xpct_goals,xpct_goals_chains,assists,keypass

## Files

Create Table.ipynb: Establishes connection with PostgreSQL and resets all the table in database

Insert_Data.ipynb: Establishes connection with PostgreSQL with the help of psycopg2 and AWS Secrets Manager.Reads data from csv and store it in dataframe via python Pandas.
                  Do some data cleaning and datatype casting.Load it into postgres database. Validate the data presence in table via test queries .Run the analysis query on it

sql_queries.py : Create,Insert,Test and Select queries which is used in the above two files

## Build ETL pipeline:

1. Run the <b>Create Table.ipynb</b> to create the database and tables
2. Run the <b>Insert_Data.ipynb</b> to load the data from csv file to PostgreSQL, run validataions and analysis
